# 01. 오프닝

## 지금까지 배운 내용 연결

- **Day 1**: 데이터 엔지니어링 개념, Linux 기본, Docker 환경 구성
- **Day 2**: HDFS — 대용량 데이터를 블록 단위로 분산 저장. NameNode, DataNode
- **Day 3**: Spark — In-memory 분산 처리 엔진. PySpark로 데이터 필터링·집계. **단발성** 분석 (CSV 읽고 → 집계 → 결과 확인)

→ 오늘은 **저장(HDFS) + 처리(Spark)**를 연결해 **완전한 데이터 파이프라인**을 만듭니다.  

**Day 3과의 차이**: Day 3에서는 **한 번** 데이터를 읽고 분석했다면, 오늘은 **반복·자동화 가능한 파이프라인**을 만듭니다. 즉, **원시 데이터 → 처리 → 결과 저장 → 다시 활용**하는 **연속적인 흐름**을 구축합니다.

---

## 오늘의 학습 목표

오늘 강의를 마치면:
1. 배치 처리의 개념과 필요성을 이해한다
2. Data Warehouse와 Data Lake의 차이를 설명할 수 있다
3. ETL, ELT, ETLT의 차이를 이해하고 적절한 방식을 선택할 수 있다
4. 데이터 파이프라인을 설계하고 자동화할 수 있다
5. ETLT 파이프라인을 직접 구현할 수 있다 (개인정보 보호, 스테이징, 검증 포함)

---

## 📝 오늘의 진행 순서

1. [01_오프닝.md](01_오프닝.md) - 오프닝 및 Day1-3 복습 (지금 여기!)
2. [02_배치처리_개념.md](02_배치처리_개념.md) - 배치 처리란?
3. [03_파이프라인_설계.md](03_파이프라인_설계.md) - 파이프라인 설계 원칙
4. [04_Data_Warehouse_Data_Lake.md](04_Data_Warehouse_Data_Lake.md) - DW vs DL 개념
5. [05_ETL_프로세스.md](05_ETL_프로세스.md) - ETL/ELT/ETLT 이해
6. [06_파이프라인_자동화.md](06_파이프라인_자동화.md) - 파이프라인 자동화
7. [07_파이프라인_검증.md](07_파이프라인_검증.md) - 파이프라인 검증
8. [08_실전_ETLT_파이프라인.md](08_실전_ETLT_파이프라인.md) - 종합 실습 과제
9. [09_QA_마무리.md](09_QA_마무리.md) - Q&A 및 마무리

---

## 준비물 확인

- [ ] Day 1~3에서 구성한 환경 (HDFS, Spark)
- [ ] 최소 30GB 여유 디스크 공간
- [ ] Python 기본 문법 이해
