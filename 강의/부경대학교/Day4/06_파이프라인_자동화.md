# 06. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë° ìë™í™”

## ì‹¤ìŠµ ëª©í‘œ

ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ **í•˜ë‚˜ì˜ ìë™í™”ëœ ì½”ë“œ**ë¡œ ë§Œë“¤ê³ , **ì—ëŸ¬ ì²˜ë¦¬Â·ë¡œê¹…Â·ì¬ì‚¬ìš©ì„±**ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

**Day3ê³¼ì˜ ì°¨ì´ì **:
- Day3: ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ ì„œ ëŒ€í™”í˜• ì‹¤í–‰ (íƒìƒ‰ì  ë¶„ì„)
- Day4: ì „ì²´ë¥¼ í•˜ë‚˜ì˜ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ í†µí•© + ì—ëŸ¬ ì²˜ë¦¬ + ë¡œê¹… (ìë™í™”Â·ìš´ì˜)

---

## ì‹¤ìŠµ ë‹¨ê³„

### 1. í™˜ê²½ í™•ì¸ (Day2 HDFS + Day3 Spark)

Day2ì—ì„œ êµ¬ì„±í•œ HDFSì™€ Day3 Sparkê°€ ëª¨ë‘ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

```bash
# HDFS ì»¨í…Œì´ë„ˆ í™•ì¸
podman ps | grep namenode
podman ps | grep datanode

# Spark ì»¨í…Œì´ë„ˆ í™•ì¸
podman ps | grep spark

# ëª¨ë‘ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹ˆë¼ë©´
cd ~/Desktop/data-engineering/day2
podman compose up -d

cd ~/Desktop/data-engineering/day3
podman compose up -d
```

### 2. HDFSì— ìƒ˜í”Œ ë°ì´í„° ì—…ë¡œë“œ

Day3ì—ì„œ ì‚¬ìš©í•œ `users.csv` ë°ì´í„°ë¥¼ HDFSì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.

```bash
# ë¡œì»¬ì— ë°ì´í„° ìƒì„± (ì´ë¯¸ ìˆë‹¤ë©´ ê±´ë„ˆë›°ê¸°)
cat > ~/Desktop/data-engineering/day3/data/users.csv << 'EOF'
id,name,age,city,salary
1,Alice,25,Seoul,50000
2,Bob,30,Busan,60000
3,Charlie,35,Incheon,70000
4,David,28,Seoul,55000
5,Eve,32,Busan,65000
EOF

# HDFSì— ë””ë ‰í† ë¦¬ ìƒì„± ë° ì—…ë¡œë“œ
podman exec namenode hdfs dfs -mkdir -p /user/data/input
podman exec namenode hdfs dfs -chmod 777 /user/data
podman cp ~/Desktop/data-engineering/day3/data/users.csv namenode:/tmp/
podman exec namenode hdfs dfs -put -f /tmp/users.csv /user/data/input/

# ì—…ë¡œë“œ í™•ì¸
podman exec namenode hdfs dfs -ls /user/data/input/
```

### 3. Jupyter Notebookì—ì„œ ìë™í™”ëœ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

1. ë¸Œë¼ìš°ì €ì—ì„œ **http://localhost:8888** ì ‘ì†
2. ìƒˆ ë…¸íŠ¸ë¶ ìƒì„± (Python 3)
3. ë‹¤ìŒ ì½”ë“œë¥¼ **í•˜ë‚˜ì˜ ì…€**ì— ì…ë ¥í•˜ê³  ì‹¤í–‰ (`Shift + Enter`):

```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from datetime import datetime

def run_etl_pipeline(input_path, output_base_path):
    """
    ETL íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜
    
    Args:
        input_path: HDFS ì…ë ¥ ê²½ë¡œ
        output_base_path: HDFS ì¶œë ¥ ê¸°ë³¸ ê²½ë¡œ
    
    Returns:
        output_path: ì‹¤ì œ ì €ì¥ëœ ê²½ë¡œ
    """
    try:
        # SparkSession ìƒì„±
        print("ğŸš€ Starting ETL Pipeline...")
        spark = SparkSession.builder \
            .appName("ETL Pipeline") \
            .master("spark://spark-master:7077") \
            .getOrCreate()
        
        # Extract
        print(f"\nğŸ“¥ [1/3] Extracting data from HDFS...")
        print(f"    Input: {input_path}")
        df = spark.read.csv(input_path, header=True, inferSchema=True)
        row_count = df.count()
        print(f"    âœ… Loaded {row_count} rows")
        
        if row_count == 0:
            raise ValueError("No data found in input file")
        
        # Transform
        print(f"\nğŸ”„ [2/3] Transforming data...")
        city_stats = df.groupBy("city").agg(
            F.count("id").alias("total_users"),
            F.avg("age").alias("avg_age"),
            F.avg("salary").alias("avg_salary")
        )
        group_count = city_stats.count()
        print(f"    âœ… Transformed into {group_count} groups")
        
        # Load
        print(f"\nğŸ’¾ [3/3] Loading data to HDFS...")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"{output_base_path}/city_stats_{timestamp}"
        print(f"    Output: {output_path}")
        
        city_stats.write.parquet(output_path, mode="overwrite")
        print(f"    âœ… Saved {group_count} groups")
        
        # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        print("\n" + "="*50)
        print("ğŸ“Š Results Preview")
        print("="*50)
        city_stats.show()
        
        print("\nğŸ‰ Pipeline completed successfully!")
        print(f"ğŸ“ Output location: {output_path}")
        
        return output_path
        
    except Exception as e:
        print(f"\nâŒ Pipeline failed: {str(e)}")
        raise

# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
input_path = "hdfs://namenode:8020/user/data/input/users.csv"
output_base = "hdfs://namenode:8020/user/data/output"

output_path = run_etl_pipeline(input_path, output_base)
```

**ì˜ˆìƒ ì¶œë ¥**:

```
ğŸš€ Starting ETL Pipeline...

ğŸ“¥ [1/3] Extracting data from HDFS...
    Input: hdfs://namenode:8020/user/data/input/users.csv
    âœ… Loaded 5 rows

ğŸ”„ [2/3] Transforming data...
    âœ… Transformed into 3 groups

ğŸ’¾ [3/3] Loading data to HDFS...
    Output: hdfs://namenode:8020/user/data/output/city_stats_20260218_114500
    âœ… Saved 3 groups

==================================================
ğŸ“Š Results Preview
==================================================
+-------+-----------+-------+----------+
|   city|total_users|avg_age|avg_salary|
+-------+-----------+-------+----------+
|  Seoul|          2|   26.5|   52500.0|
|  Busan|          2|   31.0|   62500.0|
|Incheon|          1|   35.0|   70000.0|
+-------+-----------+-------+----------+

ğŸ‰ Pipeline completed successfully!
ğŸ“ Output location: hdfs://namenode:8020/user/data/output/city_stats_20260218_114500
```

### 4. ê²°ê³¼ í™•ì¸

**HDFS Web UI**ì—ì„œ í™•ì¸:
1. ë¸Œë¼ìš°ì €ì—ì„œ **http://localhost:9870** ì ‘ì†
2. "Utilities" â†’ "Browse the file system" í´ë¦­
3. `/user/data/output/` ë””ë ‰í† ë¦¬ í™•ì¸

**ë˜ëŠ” í„°ë¯¸ë„ì—ì„œ í™•ì¸**:

```bash
# HDFSì— ì €ì¥ëœ ê²°ê³¼ í™•ì¸
podman exec namenode hdfs dfs -ls /user/data/output/
podman exec namenode hdfs dfs -ls /user/data/output/city_stats_20260218_114500/
```

**ë˜ëŠ” Notebookì—ì„œ ë°”ë¡œ ì½ê¸°**:

```python
# ê°€ì¥ ìµœê·¼ ê²°ê³¼ ì½ê¸° (ìœ„ ì½”ë“œì—ì„œ ì¶œë ¥ëœ ê²½ë¡œ ì‚¬ìš©)
result_df = spark.read.parquet("hdfs://namenode:8020/user/data/output/city_stats_20260218_114500")
result_df.show()
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ: "Connection refused to namenode:8020"

**ì›ì¸**: 
- HDFS NameNodeê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹ˆê±°ë‚˜, ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œ.

**í•´ê²°**:

```bash
# HDFS ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
podman ps | grep namenode
podman ps | grep datanode

# ì»¨í…Œì´ë„ˆê°€ ì—†ë‹¤ë©´ Day2 í™˜ê²½ ì¬ì‹œì‘
cd ~/Desktop/data-engineering/day2
podman compose up -d

# ë„¤íŠ¸ì›Œí¬ í™•ì¸ (Day2, Day3 ê°™ì€ ë„¤íŠ¸ì›Œí¬ì— ìˆì–´ì•¼ í•¨)
podman network inspect data-engineering-network
```

### ë¬¸ì œ: "Connection refused to spark-master:7077"

**ì›ì¸**: Spark Masterê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹ˆê±°ë‚˜, ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œ.

**í•´ê²°**:

```bash
# Spark Master ìƒíƒœ í™•ì¸
podman ps | grep spark-master

# ì»¨í…Œì´ë„ˆê°€ ì—†ë‹¤ë©´ Day3 í™˜ê²½ ì¬ì‹œì‘
cd ~/Desktop/data-engineering/day3
podman compose up -d
```

---

## ì²´í¬í¬ì¸íŠ¸

**"íŒŒì´í”„ë¼ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆë‚˜ìš”?"**

---

## Day3 vs Day4: ë¬´ì—‡ì´ ë‹¤ë¥¸ê°€?

| êµ¬ë¶„ | Day3 (íƒìƒ‰ì  ë¶„ì„) | Day4 (íŒŒì´í”„ë¼ì¸ ìë™í™”) |
|------|-------------------|------------------------|
| **ëª©ì ** | ë°ì´í„° íƒìƒ‰Â·í•™ìŠµ | ìš´ì˜Â·ìë™í™” |
| **ì‹¤í–‰ ë°©ì‹** | ë‹¨ê³„ë³„ ì…€ ì‹¤í–‰ | í•˜ë‚˜ì˜ í•¨ìˆ˜ë¡œ í†µí•© |
| **ì—ëŸ¬ ì²˜ë¦¬** | ì—†ìŒ | try-exceptë¡œ ì—ëŸ¬ ì²˜ë¦¬ |
| **ë¡œê¹…** | ê°„ë‹¨í•œ print | ë‹¨ê³„ë³„ ìƒì„¸ ë¡œê¹… (1/3, 2/3...) |
| **ì¬ì‚¬ìš©ì„±** | ë‚®ìŒ (ë§¤ë²ˆ ë³µì‚¬) | ë†’ìŒ (í•¨ìˆ˜ë¡œ ì¬ì‚¬ìš©) |
| **ê²€ì¦** | ì—†ìŒ | ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸ |
| **ì¶œë ¥** | ê°„ë‹¨í•œ ê²°ê³¼ | êµ¬ì¡°í™”ëœ ë¡œê·¸ + ê²½ë¡œ ë°˜í™˜ |

---

## í•µì‹¬ ê°œë… ì •ë¦¬

- **í•¨ìˆ˜í™”**: ETL ë¡œì§ì„ í•¨ìˆ˜ë¡œ ë§Œë“¤ì–´ ì¬ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ í•¨.
- **ì—ëŸ¬ ì²˜ë¦¬**: try-exceptë¡œ ì‹¤íŒ¨ ì‹œ ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€ ì œê³µ.
- **ë¡œê¹…**: ê° ë‹¨ê³„(1/3, 2/3, 3/3)ì˜ ì§„í–‰ ìƒí™©ê³¼ ê²°ê³¼ë¥¼ ëª…í™•íˆ ê¸°ë¡.
- **ê²€ì¦**: ì…ë ¥ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (row_count == 0 ì²´í¬).
- **íƒ€ì„ìŠ¤íƒ¬í”„ ì¶œë ¥**: ì‹¤í–‰ ì´ë ¥ ë³´ì¡´ + ë¡¤ë°± ê°€ëŠ¥.
- **ë‹¤ìŒ ë‹¨ê³„**: Python ìŠ¤í¬ë¦½íŠ¸(`.py`)ë¡œ ì €ì¥ â†’ `spark-submit`ìœ¼ë¡œ ì‹¤í–‰ â†’ Airflow/Dagsterë¡œ ìŠ¤ì¼€ì¤„ë§.

---

## ì°¸ê³ 

- [07_íŒŒì´í”„ë¼ì¸_ê²€ì¦.md](07_íŒŒì´í”„ë¼ì¸_ê²€ì¦.md) â€” ë‹¤ìŒ ë‹¨ê³„: íŒŒì´í”„ë¼ì¸ ê²€ì¦.

---

## ê·¸ë¦¼ ì¶œì²˜

ë³¸ ë¬¸ì„œì—ëŠ” ë³„ë„ ì´ë¯¸ì§€ê°€ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
