# 09. Q&A 및 마무리

## 오늘 배운 내용 요약

### 핵심 개념

1. **배치 처리와 스케줄링**
   - 배치 처리: 일정 주기마다 쌓인 데이터를 한 번에 처리
   - 실시간 vs 배치: 지연 허용도, 데이터 크기, 비용에 따라 선택
   - 스케줄링 도구: Cron, Apache Airflow, Dagster

2. **데이터 파이프라인 설계**
   - 구성 요소: Source → Ingestion → Storage → Processing → Serving
   - 설계 원칙: 멱등성, 재실행 가능성, 모니터링, 로깅
   - 네이버 사례: IDC Seamless HDFS, JuiceFS, HBase, Spark/Hive/Flink 선택 가능

3. **Data Warehouse vs Data Lake**
   - Data Warehouse 📦: 쿠팡 물류 창고처럼 정리된 창고. 빠른 검색, 높은 품질
   - Data Lake 🏞️: 호수처럼 거대하고 다양한 것을 담음. 형식 상관없이 저장
   - Data Lakehouse: Warehouse + Lake 장점 결합 (Delta Lake, Iceberg)

4. **ETL/ELT/ETLT**
   - ETL: Extract → Transform → Load (중간 서버에서 변환)
   - ELT: Extract → Load → Transform (목적지에서 변환)
   - ETLT: Extract → light Transform → Load → full Transform (현대적 접근, 보안+속도+유연성)

5. **파이프라인 자동화 및 검증**
   - 함수화: 재사용 가능한 파이프라인 함수
   - 에러 처리: try-except + traceback
   - 로깅: 단계별 진행 상황 기록
   - 검증: Null 체크, 중복 체크, 비즈니스 로직 검증
   - 실무 도구: Deequ, Great Expectations

6. **실전 ETLT 파이프라인 구현**
   - Extract + light Transform: 개인정보 마스킹 (이메일 마스킹, 민감 컬럼 삭제)
   - Load to Staging: 중간 저장소에 안전한 데이터 보관
   - full Transform: 비즈니스 로직 적용 (집계, 통계)
   - Validation: 데이터 품질 검증 후 최종 저장
   - 실무 적용: 개인정보 보호법(GDPR, CCPA) 준수

---

## Q&A

**자유 질문 시간** — 시간이 남으면 자유롭게 질문받습니다.

- 오늘 다룬 내용·실습·다음 날 준비뿐 아니라
- **네이버 회사 생활은 어떤지**
- **개발자로써 성장하려면 어떻게 해야 하는지**
- **회사 취업 꿀팁이 있는지**
- **오픈소스 메인테이너는 어떻게 됐는지**
- **해외 어디서 살았는지**

등등 편하게 질문 받습니다.

---

### 자주 나오는 질문 (FAQ)

**Q: ETL과 ELT 중 어떤 걸 써야 하나요?**  
A: **상황에 따라 다릅니다**. 온프레미스 환경이거나 목적지 리소스가 제한적이면 **ETL**. 클라우드 DW(BigQuery, Snowflake)를 쓰고 컴퓨팅을 유연하게 확장할 수 있으면 **ELT**. 보안·규정 준수가 중요하면 **ETLT** (민감 데이터를 적재 전 처리).

**Q: Data Lake에 모든 데이터를 넣으면 관리가 안 되지 않나요?**  
A: 맞습니다. 그래서 **Data Swamp**(데이터 늪)가 되지 않도록 **메타데이터 관리, 데이터 카탈로그, 거버넌스**가 중요합니다. 실무에서는 Apache Atlas, AWS Glue Data Catalog 등을 사용합니다.

**Q: 파이프라인 자동화는 왜 중요한가요?**  
A: **수동 실행은 실수가 많고, 반복 작업이 비효율적**입니다. 자동화하면 정해진 시간에 안정적으로 실행되고, 에러 발생 시 알림을 받을 수 있습니다. Airflow, Dagster로 스케줄링하면 수백 개의 파이프라인도 관리 가능합니다.

**Q: 실무에서 파이프라인 실패하면 어떻게 하나요?**  
A: **모니터링·알림 시스템**이 필수입니다. Airflow는 실패 시 이메일·Slack 알림을 보내고, 재시도 정책을 설정할 수 있습니다. 로그를 확인하여 원인을 파악하고, 데이터를 재처리합니다. **멱등성**이 있어야 재실행이 안전합니다.

**Q: Parquet vs ORC 어떤 걸 쓰나요?**  
A: **Parquet**은 Spark, Pandas, Arrow 등 범용적으로 사용됩니다. **ORC**는 Hive에 최적화되어 있습니다. 실무에서는 Spark 중심이면 Parquet, Hive 중심이면 ORC를 많이 씁니다.

**Q: 네이버에서는 어떤 파이프라인 도구를 쓰나요?**  
A: 자체 플랫폼을 운영하고 있으며, Spark, Hive, Flink 등 다양한 처리 엔진을 제공합니다. 사용자가 Python(Spark), SQL(Hive) 등 코드에 맞게 엔진을 선택할 수 있습니다. 스케줄링은 자체 시스템을 사용합니다.

**Q: 08번 실전 실습에서 막히면 어떻게 하나요?**  
A: **앞에서 배운 내용을 복습**하세요. 06번(파이프라인 자동화), 07번(검증)에 힌트가 많습니다. 정규표현식이 어려우면 힌트를 펼쳐보세요. 그래도 막히면 강사에게 질문하거나, 과제 1~3만 완성하고 4~5는 나중에 도전해도 괜찮습니다.

**Q: 스테이징 영역은 왜 필요한가요?**  
A: **개인정보 보호와 데이터 품질 관리**를 위해 필요합니다. 원시 데이터에서 민감 정보를 제거한 후 스테이징에 저장하면, 이후 단계에서는 안전한 데이터만 사용할 수 있습니다. 또한 문제가 생겼을 때 원시 데이터를 다시 처리하지 않고 스테이징부터 재시작할 수 있습니다.

**Q: Day5 전에 뭘 해두면 좋을까요?**  
A: Day 1~4 내용을 복습하세요. 특히 **Linux 기본 명령어, Docker/Podman, HDFS 명령어, PySpark 기본 문법**을 다시 보면 좋습니다. **08번 실전 실습을 완성하지 못했다면 꼭 완료**하세요. Day5에서는 전체 내용을 통합한 종합 실습을 진행합니다.

---

## 다음 시간 예고

**Day 5: 복습 & 종합 실습**

4일간 배운 내용을 모두 활용해 **완전한 데이터 파이프라인**을 구축합니다.

- Day 1~4 핵심 내용 리뷰
- 전자상거래 로그 분석 파이프라인 구축
- 질문 정리 및 해소
- 결과물 검토 및 추가 학습
- 수료 및 마무리

---

## 수고하셨습니다!

오늘 데이터 파이프라인 구축의 기초를 배웠습니다. 내일은 5일간의 내용을 종합 실습으로 마무리합니다!
