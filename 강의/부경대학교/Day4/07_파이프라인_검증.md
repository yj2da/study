# 07. íŒŒì´í”„ë¼ì¸ ê²€ì¦ ë° ê°œì„ 

## ì‹¤ìŠµ ëª©í‘œ

06ë²ˆì—ì„œ ë§Œë“  íŒŒì´í”„ë¼ì¸ì— **ë°ì´í„° í’ˆì§ˆ ê²€ì¦, ì„±ëŠ¥ ìµœì í™”, ë” ë‚˜ì€ ì—ëŸ¬ ì²˜ë¦¬**ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

---

## ì‹¤ìŠµ ë‹¨ê³„

### 1. ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì¶”ê°€

06ë²ˆ íŒŒì´í”„ë¼ì¸ì— ê²€ì¦ ë¡œì§ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from datetime import datetime

def validate_data(df, stage_name):
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
    print(f"\nğŸ” Validating {stage_name}...")

    # ê¸°ë³¸ í†µê³„
    row_count = df.count()
    col_count = len(df.columns)
    print(f"    Rows: {row_count}, Columns: {col_count}")

    # Null ì²´í¬
    null_counts = df.select([
        F.sum(F.col(c).isNull().cast("int")).alias(c)
        for c in df.columns
    ])
    print(f"    Null values:")
    null_counts.show()

    # ì¤‘ë³µ ì²´í¬
    duplicate_count = df.count() - df.dropDuplicates().count()
    if duplicate_count > 0:
        print(f"    âš ï¸  Found {duplicate_count} duplicate rows")
    else:
        print(f"    âœ… No duplicates")

    return row_count > 0

def run_etl_pipeline_with_validation(input_path, output_base_path):
    """ê²€ì¦ ë¡œì§ì´ ì¶”ê°€ëœ ETL íŒŒì´í”„ë¼ì¸"""
    try:
        print("ğŸš€ Starting ETL Pipeline with Validation...")

        # SparkSession ìƒì„±
        spark = SparkSession.builder \
            .appName("ETL Pipeline with Validation") \
            .master("spark://spark-master:7077") \
            .getOrCreate()

        # Extract
        print(f"\nğŸ“¥ [1/3] Extracting data from HDFS...")
        print(f"    Input: {input_path}")
        df = spark.read.csv(input_path, header=True, inferSchema=True)

        # ì…ë ¥ ë°ì´í„° ê²€ì¦
        if not validate_data(df, "Input Data"):
            raise ValueError("Input data validation failed: No data found")

        # Transform
        print(f"\nğŸ”„ [2/3] Transforming data...")
        city_stats = df.groupBy("city").agg(
            F.count("id").alias("total_users"),
            F.avg("age").alias("avg_age"),
            F.avg("salary").alias("avg_salary"),
            F.min("age").alias("min_age"),
            F.max("age").alias("max_age")
        )

        # ë³€í™˜ ê²°ê³¼ ê²€ì¦
        if not validate_data(city_stats, "Transformed Data"):
            raise ValueError("Transformation validation failed")

        # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ê²€ì¦
        print(f"\nğŸ” Business Logic Validation...")
        invalid_stats = city_stats.filter(
            (F.col("avg_age") < 0) | (F.col("avg_salary") < 0)
        )
        if invalid_stats.count() > 0:
            print(f"    âš ï¸  Found invalid statistics:")
            invalid_stats.show()
            raise ValueError("Invalid statistics detected")
        print(f"    âœ… All statistics are valid")

        # Load
        print(f"\nğŸ’¾ [3/3] Loading data to HDFS...")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"{output_base_path}/city_stats_{timestamp}"
        print(f"    Output: {output_path}")

        city_stats.write.parquet(output_path, mode="overwrite")

        # ì €ì¥ ê²°ê³¼ ê²€ì¦
        print(f"\nğŸ” Verifying saved data...")
        saved_df = spark.read.parquet(output_path)
        saved_count = saved_df.count()
        expected_count = city_stats.count()

        if saved_count == expected_count:
            print(f"    âœ… Verification passed: {saved_count} rows saved")
        else:
            raise ValueError(f"Verification failed: expected {expected_count}, got {saved_count}")

        # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        print("\n" + "="*50)
        print("ğŸ“Š Results Preview")
        print("="*50)
        city_stats.show()

        print("\nğŸ‰ Pipeline completed successfully!")
        print(f"ğŸ“ Output location: {output_path}")

        return output_path

    except Exception as e:
        print(f"\nâŒ Pipeline failed: {str(e)}")
        import traceback
        traceback.print_exc()
        raise

# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
input_path = "hdfs://namenode:8020/user/data/input/users.csv"
output_base = "hdfs://namenode:8020/user/data/output"

output_path = run_etl_pipeline_with_validation(input_path, output_base)
```

**ì˜ˆìƒ ì¶œë ¥**:

```
ğŸš€ Starting ETL Pipeline with Validation...

ğŸ“¥ [1/3] Extracting data from HDFS...
    Input: hdfs://namenode:8020/user/data/input/users.csv

ğŸ” Validating Input Data...
    Rows: 5, Columns: 5
    Null values:
+---+----+---+----+------+
| id|name|age|city|salary|
+---+----+---+----+------+
|  0|   0|  0|   0|     0|
+---+----+---+----+------+
    âœ… No duplicates

ğŸ”„ [2/3] Transforming data...

ğŸ” Validating Transformed Data...
    Rows: 3, Columns: 6
    Null values:
+-------+-----------+-------+----------+-------+-------+
|   city|total_users|avg_age|avg_salary|min_age|max_age|
+-------+-----------+-------+----------+-------+-------+
|      0|          0|      0|         0|      0|      0|
+-------+-----------+-------+----------+-------+-------+
    âœ… No duplicates

ğŸ” Business Logic Validation...
    âœ… All statistics are valid

ğŸ’¾ [3/3] Loading data to HDFS...
    Output: hdfs://namenode:8020/user/data/output/city_stats_20260218_120000

ğŸ” Verifying saved data...
    âœ… Verification passed: 3 rows saved

==================================================
ğŸ“Š Results Preview
==================================================
+-------+-----------+-------+----------+-------+-------+
|   city|total_users|avg_age|avg_salary|min_age|max_age|
+-------+-----------+-------+----------+-------+-------+
|Incheon|          1|   35.0|   70000.0|     35|     35|
|  Busan|          2|   31.0|   62500.0|     30|     32|
|  Seoul|          2|   26.5|   52500.0|     25|     28|
+-------+-----------+-------+----------+-------+-------+

ğŸ‰ Pipeline completed successfully!
ğŸ“ Output location: hdfs://namenode:8020/user/data/output/city_stats_20260218_120000
```

---

### 2. ì„±ëŠ¥ ìµœì í™”

**ìºì‹± í™œìš©**:

```python
# ì—¬ëŸ¬ ë²ˆ ì‚¬ìš©í•˜ëŠ” DataFrameì€ ìºì‹±
df.cache()
df.count()  # ì²« ì‹¤í–‰ ì‹œ ìºì‹œì— ì €ì¥
df.show()   # ì´í›„ ì‹¤í–‰ì€ ìºì‹œì—ì„œ ì½ìŒ

# ì‚¬ìš© í›„ ìºì‹œ í•´ì œ
df.unpersist()
```

**íŒŒí‹°ì…”ë‹ (ëŒ€ìš©ëŸ‰ ë°ì´í„° ì‹œ)**:

```python
# ë„ì‹œë³„ë¡œ íŒŒí‹°ì…˜ (ì¿¼ë¦¬ ì‹œ íŠ¹ì • ë„ì‹œë§Œ ì½ì„ ìˆ˜ ìˆìŒ)
city_stats.write.partitionBy("city").parquet(output_path, mode="overwrite")

# ì½ì„ ë•Œ
spark.read.parquet(output_path).filter(F.col("city") == "Seoul").show()
```

**Coalesceë¡œ íŒŒì¼ ìˆ˜ ì¤„ì´ê¸°**:

```python
# ì‘ì€ ë°ì´í„°ëŠ” ë‹¨ì¼ íŒŒì¼ë¡œ ì €ì¥ (íŒŒì¼ ìˆ˜ ì¤„ì´ê¸°)
city_stats.coalesce(1).write.parquet(output_path, mode="overwrite")
```

---

## ì²´í¬í¬ì¸íŠ¸

**"ê²€ì¦ ë¡œì§ì´ ì¶”ê°€ë˜ì–´ ë°ì´í„° í’ˆì§ˆì„ í™•ì¸í•  ìˆ˜ ìˆë‚˜ìš”?"**

---

## í•µì‹¬ ê°œë… ì •ë¦¬

- **ë°ì´í„° í’ˆì§ˆ ê²€ì¦**:
  - í–‰/ì—´ ìˆ˜ í™•ì¸
  - Null ê°’ ì²´í¬
  - ì¤‘ë³µ ì²´í¬
  - ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ê²€ì¦ (ìŒìˆ˜ ê°’ ë“±)
  - ì €ì¥ í›„ ì¬í™•ì¸
- **ì„±ëŠ¥ ìµœì í™”**:
  - ìºì‹±: ë°˜ë³µ ì‚¬ìš© DataFrame
  - íŒŒí‹°ì…”ë‹: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì¿¼ë¦¬ ìµœì í™”
  - Coalesce: ì‘ì€ íŒŒì¼ í†µí•©
- **ì—ëŸ¬ ì²˜ë¦¬**: try-except + tracebackìœ¼ë¡œ ìƒì„¸ ì—ëŸ¬ ì •ë³´ ì œê³µ

---

## ì‹¤ë¬´ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°ì´í„° ê²€ì¦ ë„êµ¬ (ì°¸ê³ )

ì‹¤ìŠµì—ì„œëŠ” ì§ì ‘ ê²€ì¦ ë¡œì§ì„ ì‘ì„±í–ˆì§€ë§Œ, ì‹¤ë¬´ì—ì„œëŠ” ì „ë¬¸ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤.

### Deequ vs Great Expectations

| í•­ëª©                 | Deequ                           | Great Expectations                            |
| -------------------- | ------------------------------- | --------------------------------------------- |
| **ê¸°ë°˜**             | Spark ë¼ì´ë¸ŒëŸ¬ë¦¬                | Python íŒ¨í‚¤ì§€                                 |
| **ì–¸ì–´**             | Scala, Python (pydeequ)         | Python                                        |
| **ë°ì´í„° ì†ŒìŠ¤**      | Sparkì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•œ ëª¨ë“  ì†ŒìŠ¤ | íŒŒì¼ ì‹œìŠ¤í…œ, SQL DB, ì¸ë©”ëª¨ë¦¬ (pandas, Spark) |
| **ë¬¸ì„œí™”Â·ì»¤ë®¤ë‹ˆí‹°**  | ìƒëŒ€ì ìœ¼ë¡œ ë¯¸ì•½í•¨               | ìƒëŒ€ì ìœ¼ë¡œ í™œë°œí•¨                             |
| **ëŸ¬ë‹ ì»¤ë¸Œ**        | Sparkë¥¼ ì•Œì•„ì•¼ í•¨               | ìŠµë“í•  ìƒˆë¡œìš´ ìš©ì–´Â·ê°œë…ì´ ë§ìŒ                |
| **ë°ì´í„° í’ˆì§ˆ ê·œì¹™** | ìƒëŒ€ì ìœ¼ë¡œ ì ìŒ                 | ìƒëŒ€ì ìœ¼ë¡œ ë§ê³ , í™œë°œí•˜ê²Œ ê¸°ì—¬                |
| **Integration**      | AWS Glue ë“±                     | Airflow, Databricks ë“±                        |
| **ì¶”ì²œ ìƒí™©**        | Spark ì¤‘ì‹¬ í™˜ê²½                 | Python ì¤‘ì‹¬, ë‹¤ì–‘í•œ ì†ŒìŠ¤                      |

### ê°œì¸ì  ì„ í˜¸: Great Expectations

- **ì´ìœ **:
  - ë¬¸ì„œí™”ê°€ ì˜ ë˜ì–´ ìˆê³ , ì»¤ë®¤ë‹ˆí‹°ê°€ í™œë°œí•¨.
  - ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ ì§€ì›.
  - ë°ì´í„° í’ˆì§ˆ ê·œì¹™ì´ í’ë¶€í•˜ê³ , ê³„ì† ì¶”ê°€ë¨.
- **ë‹¨ì **:
  - ëŸ¬ë‹ ì»¤ë¸Œê°€ ìˆìŒ (Expectation, Checkpoint, Data Context ë“± ìƒˆë¡œìš´ ê°œë…).

### ì–¸ì œ ì‚¬ìš©í•˜ë‚˜?

- **ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ íŒŒì´í”„ë¼ì¸**: ë°ì´í„° í’ˆì§ˆì´ ì¤‘ìš”í•  ë•Œ.
- **ê·œì • ì¤€ìˆ˜**: ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ê°€ í•„ìš”í•  ë•Œ.
- **ë³µì¡í•œ ê²€ì¦**: ìˆ˜ì‹­ ê°œì˜ ê·œì¹™ì„ ê´€ë¦¬í•´ì•¼ í•  ë•Œ.

**ì´ë²ˆ ê°•ì˜ì—ì„œëŠ” ê¸°ë³¸ ê²€ì¦ ë¡œì§ì„ ì§ì ‘ ì‘ì„±**í•´ë´¤ì§€ë§Œ, ì‹¤ë¬´ì—ì„œëŠ” ì´ëŸ° ë„êµ¬ë“¤ì„ í™œìš©í•˜ë©´ ë” íš¨ìœ¨ì ì…ë‹ˆë‹¤.

---

## ì°¸ê³ 

- [Deequ - AWS Labs](https://github.com/awslabs/deequ)
- [Great Expectations](https://greatexpectations.io/)
- [ë„¤ì´ë²„ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‚¬ë¡€ - NAVER D2](https://d2.naver.com/helloworld/5766317)
- [08_ì‹¤ì „_ETLT_íŒŒì´í”„ë¼ì¸.md](08_ì‹¤ì „_ETLT_íŒŒì´í”„ë¼ì¸.md) â€” ë‹¤ìŒ ë‹¨ê³„: ì‹¤ì „ ETLT íŒŒì´í”„ë¼ì¸ êµ¬ì¶•.

---

## ê·¸ë¦¼ ì¶œì²˜

ë³¸ ë¬¸ì„œì—ëŠ” ë³„ë„ ì´ë¯¸ì§€ê°€ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
