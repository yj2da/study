# 08. 분산 저장 시나리오 실습

## 실습 목표

**로그 파일**을 날짜별 디렉토리 구조로 HDFS에 올리고, **용량·블록 정보**를 확인합니다.

---

## 시나리오: 로그 파일 저장 및 관리

### 1. 로그 파일 생성 (로컬)

```bash
cd ~/Desktop/data-engineering/day2

for i in {1..5}; do
  echo "2024-02-14 13:00:0$i INFO User login: user$i" > log_$i.txt
done
```

### 2. HDFS에 디렉토리 구조 만들고 업로드

(NameNode 컨테이너에 로그 파일을 넣은 뒤, 또는 호스트에서 `podman cp`로 복사한 뒤)

```bash
# 컨테이너 안에서 (로그 파일이 /tmp에 있다고 가정)
hdfs dfs -mkdir -p /logs/2026/02/14
hdfs dfs -put /tmp/log_*.txt /logs/2026/02/14/
# 또는 put 전에 podman cp로 namenode:/tmp/ 에 넣기
for f in log_*.txt; do podman cp "$f" namenode:/tmp/; done

hdfs dfs -ls -R /logs/
```

### 3. 파일 통계·블록 확인

```bash
hdfs dfs -du -h /logs/
hdfs dfs -count /logs/
hdfs fsck /logs/ -files -blocks -locations
```

- `-du -h`: 디렉토리별 용량
- `-count`: 파일·디렉토리 개수
- `fsck`: 파일·블록·위치 점검

---

## 핵심 개념 정리

- **경로 구조**: `/logs/연/월/일` 처럼 날짜별로 나누면 나중에 기간별 조회·정리하기 쉽습니다.
- **du, count, fsck**: 용량·개수·블록 배치를 확인할 때 유용합니다.

---

## 추가 실습 (10~15분)

아래 **한글 지시**를 보고, HDFS 명령어를 직접 찾아서 수행해 보세요.  
**Cursor, ChatGPT 등 LLM 도움을 받아도 괜찮습니다.** 단, `hdfs dfs` 명령어가 **각각 무슨 역할을 하는지 이해**하고 사용하세요.

---

### 미션 A: 로그 분석 파이프라인

```
1. 로컬에서 ERROR, WARN, INFO가 섞인 로그 파일(최소 20줄)을 만드세요
2. HDFS에 /logs/본인이름/raw/ 디렉토리를 만들고 업로드하세요
3. HDFS에서 ERROR가 포함된 줄만 찾아서 로컬에 error_only.txt로 저장하세요
4. /logs/본인이름/processed/ 디렉토리를 만들고 error_only.txt를 업로드하세요
5. raw와 processed 폴더의 용량을 비교하세요
```

---

### 미션 B: 멀티 파일 배치 처리

```
1. 로컬에서 sensor_01.txt ~ sensor_05.txt 파일 5개를 만드세요
   (각 파일에 센서ID, 온도, 습도 데이터 3줄 이상)
2. HDFS에 /iot/본인이름/2026/02/16/ 경로를 만들고 모든 파일을 업로드하세요
3. 전체 파일 개수와 총 용량을 한 번에 확인하세요
4. 온도가 30 이상인 줄만 찾아서 출력하세요 (모든 파일 대상)
5. sensor_03.txt만 로컬로 다운로드하세요
```

---

### 완료 후

다 했으면 아래 명령어를 실행해서 **결과를 강사에게 보여주세요**.

```bash
# 미션 A 확인
hdfs dfs -ls -R /logs/
hdfs dfs -cat /logs/본인이름/processed/error_only.txt

# 미션 B 확인
hdfs dfs -ls -R /iot/
hdfs dfs -count /iot/본인이름/

# 사용한 명령어 확인
history | tail -20
```

---

## 참고

- [09_QA_마무리.md](09_QA_마무리.md) — 오늘 정리·Q&A.

---

## 그림 출처

본 문서에는 인용한 외부 그림이 없습니다.
