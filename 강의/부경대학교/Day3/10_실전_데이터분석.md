# 10. ì¢…í•© ì‹¤ìŠµ ê³¼ì œ

## ì‹¤ìŠµ ëª©í‘œ

ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ë‚´ìš©ì„ í™œìš©í•˜ì—¬ **ì „ììƒê±°ë˜ ì£¼ë¬¸ ë°ì´í„°**ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

> ğŸ’¡ **ì´ë²ˆ ì‹¤ìŠµì€ ì—¬ëŸ¬ë¶„ì´ ì§ì ‘ í•´ë³´ëŠ” ì‹œê°„ì…ë‹ˆë‹¤!**
> - ì •ë‹µ ì½”ë“œëŠ” ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
> - **íŒíŠ¸ëŠ” ìµœëŒ€í•œ ë³´ì§€ ë§ê³ **, 2ë²ˆ ì´ìƒ ìƒê°í•´ë„ ì •ë§ ëª¨ë¥´ê² ì„ ë•Œë§Œ í™•ì¸í•˜ì„¸ìš”.
> - ì²˜ìŒì´ë¼ ì–´ë µê² ì§€ë§Œ, **ì‹¤ë¬´ì—ì„œëŠ” ìŠ¤ìŠ¤ë¡œ ìƒê°í•˜ê³  í•´ê²°í•˜ëŠ” ëŠ¥ë ¥ì´ ì¤‘ìš”**í•©ë‹ˆë‹¤.
> - ë¬¼ë¡  ì‹¤ë¬´ì—ì„œëŠ” LLM ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆì§€ë§Œ, ì´ë²ˆì€ **ì—°ìŠµ**ì´ë‹ˆ ìµœëŒ€í•œ ì™¸ë¶€ ë„ì›€ ì—†ì´ ìŠ¤ìŠ¤ë¡œ í•´ë³´ì„¸ìš”.
> - ì•ì—ì„œ ë°°ìš´ ë‚´ìš©(06~09)ì„ ì°¸ê³ í•˜ì—¬ ì§ì ‘ ì‘ì„±í•´ë³´ì„¸ìš”.
> - ì •ë§ ë§‰íˆë©´ ê°•ì‚¬ì—ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”!

---

## ì‹œë‚˜ë¦¬ì˜¤: ì „ììƒê±°ë˜ ì£¼ë¬¸ ë°ì´í„° ë¶„ì„

### 1. ë°ì´í„° ì¤€ë¹„

í„°ë¯¸ë„ì—ì„œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```bash
cat > ~/Desktop/data-engineering/day3/data/orders.csv << 'EOF'
order_id,user_id,product,quantity,price,date
1,101,Laptop,1,1000,2024-02-01
2,102,Mouse,2,20,2024-02-01
3,101,Keyboard,1,50,2024-02-02
4,103,Monitor,1,300,2024-02-02
5,102,Laptop,1,1000,2024-02-03
6,104,Mouse,3,20,2024-02-03
7,103,Keyboard,2,50,2024-02-04
8,105,Monitor,1,300,2024-02-04
9,101,Mouse,5,20,2024-02-05
10,102,Keyboard,1,50,2024-02-05
EOF
```

**ë°ì´í„° ì„¤ëª…:**
- `order_id`: ì£¼ë¬¸ ë²ˆí˜¸
- `user_id`: ê³ ê° ID
- `product`: ì œí’ˆëª…
- `quantity`: ìˆ˜ëŸ‰
- `price`: ë‹¨ê°€
- `date`: ì£¼ë¬¸ ë‚ ì§œ

---

## ê³¼ì œ

### ê³¼ì œ 1: ë°ì´í„° ë¡œë“œ ë° í™•ì¸

1. `orders.csv` íŒŒì¼ì„ DataFrameìœ¼ë¡œ ë¡œë“œí•˜ì„¸ìš”.
2. ë°ì´í„° êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš” (`show()`, `printSchema()`).
3. ì´ ëª‡ ê°œì˜ ì£¼ë¬¸ì´ ìˆë‚˜ìš”? (`count()`)

<details>
<summary>íŒíŠ¸</summary>

```python
orders = spark.read.csv("...", header=True, inferSchema=True)
```

</details>

---

### ê³¼ì œ 2: ì´ ë§¤ì¶œ ê³„ì‚°

ê° ì£¼ë¬¸ì˜ ì´ ë§¤ì¶œ(`total = quantity * price`)ì„ ê³„ì‚°í•˜ëŠ” ìƒˆ ì»¬ëŸ¼ì„ ì¶”ê°€í•˜ì„¸ìš”.

<details>
<summary>íŒíŠ¸</summary>

`withColumn()` ì‚¬ìš©

</details>

---

### ê³¼ì œ 3: ì œí’ˆë³„ ë¶„ì„

**DataFrame API ë˜ëŠ” Spark SQL ì¤‘ ì„ íƒí•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”.**

1. ì œí’ˆë³„ ì´ íŒë§¤ëŸ‰ê³¼ ì´ ë§¤ì¶œì„ ê³„ì‚°í•˜ì„¸ìš”.
2. ë§¤ì¶œì´ ë†’ì€ ìˆœì„œë¡œ ì •ë ¬í•˜ì„¸ìš”.

**ì˜ˆìƒ ê²°ê³¼:**
```
+--------+--------------+-------------+
|product |total_quantity|total_revenue|
+--------+--------------+-------------+
|Laptop  |2             |2000         |
|Monitor |2             |600          |
|Mouse   |10            |200          |
|Keyboard|4             |200          |
+--------+--------------+-------------+
```

<details>
<summary>íŒíŠ¸ (DataFrame API)</summary>

```python
from pyspark.sql import functions as F

orders.groupBy("...").agg(
    F.sum("...").alias("..."),
    F.sum(...).alias("...")
).orderBy("...", ascending=False).show()
```

</details>

<details>
<summary>íŒíŠ¸ (Spark SQL)</summary>

```python
orders.createOrReplaceTempView("orders")

spark.sql("""
    SELECT product, 
           SUM(...) as total_quantity,
           SUM(...) as total_revenue
    FROM orders
    GROUP BY ...
    ORDER BY ... DESC
""").show()
```

</details>

---

### ê³¼ì œ 4: ê³ ê°ë³„ ë¶„ì„

ê°€ì¥ ë§ì´ êµ¬ë§¤í•œ ê³ ê° TOP 3ë¥¼ ì°¾ìœ¼ì„¸ìš”.

**ì¶œë ¥ í•­ëª©:**
- `user_id`: ê³ ê° ID
- `order_count`: ì£¼ë¬¸ íšŸìˆ˜
- `total_spent`: ì´ ì§€ì¶œ ê¸ˆì•¡

**ì˜ˆìƒ ê²°ê³¼:**
```
+-------+-----------+-----------+
|user_id|order_count|total_spent|
+-------+-----------+-----------+
|101    |3          |1150       |
|102    |3          |1090       |
|103    |2          |400        |
+-------+-----------+-----------+
```

<details>
<summary>íŒíŠ¸</summary>

`groupBy("user_id")` â†’ `agg()` â†’ `orderBy()` â†’ `limit(3)`

</details>

---

### ê³¼ì œ 5: ê²°ê³¼ ì €ì¥ ë° í™•ì¸ (ì„ íƒ)

ì œí’ˆë³„ ë§¤ì¶œ ê²°ê³¼ë¥¼ HDFSì— ì €ì¥í•˜ê³ , ì €ì¥ì´ ì˜ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

**ì €ì¥ ê²½ë¡œ:** `hdfs://namenode:8020/user/data/output/product_analysis`

**í™•ì¸ ë°©ë²•:**
1. Sparkë¡œ ì €ì¥ëœ íŒŒì¼ì„ ë‹¤ì‹œ ì½ì–´ì„œ `show()`
2. í„°ë¯¸ë„ì—ì„œ HDFS ëª…ë ¹ì–´ë¡œ í™•ì¸

<details>
<summary>íŒíŠ¸</summary>

```python
# ì €ì¥
result.write.csv("hdfs://...", header=True, mode="overwrite")

# ì €ì¥ í™•ì¸ (Spark)
spark.read.csv("hdfs://...", header=True).show()
```

HDFS ê¶Œí•œ í™•ì¸:
```bash
podman exec -it namenode hdfs dfs -chmod -R 777 /user/data
```

í„°ë¯¸ë„ í™•ì¸:
```bash
podman exec -it namenode hdfs dfs -ls /user/data/output/product_analysis
```

</details>

---

## ì²´í¬í¬ì¸íŠ¸

**ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸:**

- [ ] ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆë‚˜ìš”?
- [ ] ì œí’ˆë³„ ë§¤ì¶œì„ ê³„ì‚°í–ˆë‚˜ìš”?
- [ ] ê³ ê°ë³„ ë¶„ì„ì„ ì™„ë£Œí–ˆë‚˜ìš”?
- [ ] (ì„ íƒ) HDFSì— ê²°ê³¼ë¥¼ ì €ì¥í–ˆë‚˜ìš”?

---

## ì¶”ê°€ ë„ì „ ê³¼ì œ (ì‹œê°„ì´ ë‚¨ìœ¼ë©´)

1. **ë‚ ì§œë³„ ë§¤ì¶œ ì¶”ì´**: ë‚ ì§œë³„ ì´ ë§¤ì¶œì„ ê³„ì‚°í•˜ì„¸ìš”.
2. **ê°€ì¥ ì¸ê¸° ìˆëŠ” ì œí’ˆ**: ì£¼ë¬¸ íšŸìˆ˜ê°€ ê°€ì¥ ë§ì€ ì œí’ˆì€?
3. **ê³ ê°€ ì£¼ë¬¸ í•„í„°ë§**: ì´ ë§¤ì¶œì´ 500 ì´ìƒì¸ ì£¼ë¬¸ë§Œ ì¡°íšŒí•˜ì„¸ìš”.

---

## í•µì‹¬ ê°œë… ì •ë¦¬

ì´ë²ˆ ì‹¤ìŠµì—ì„œ ì‚¬ìš©í•œ ì£¼ìš” ê°œë…:
- **ë°ì´í„° ë¡œë“œ**: `spark.read.csv()`
- **ë°ì´í„° ë³€í™˜**: `withColumn()`, `select()`
- **ì§‘ê³„**: `groupBy()`, `agg()`, `F.sum()`, `F.count()`
- **ì •ë ¬**: `orderBy()`
- **SQL**: `createOrReplaceTempView()`, `spark.sql()`
- **ì €ì¥**: `write.csv()`

---

## ì°¸ê³ 

- [06_ë°ì´í„°_ë¡œë“œ_ê¸°ë³¸ì¡°ì‘.md](06_ë°ì´í„°_ë¡œë“œ_ê¸°ë³¸ì¡°ì‘.md) â€” ë°ì´í„° ë¡œë“œ ë°©ë²•
- [07_í•„í„°ë§_ì§‘ê³„.md](07_í•„í„°ë§_ì§‘ê³„.md) â€” groupBy, agg ì‚¬ìš©ë²•
- [09_Spark_SQL.md](09_Spark_SQL.md) â€” Spark SQL ì‚¬ìš©ë²•
- [11_QA_ë§ˆë¬´ë¦¬.md](11_QA_ë§ˆë¬´ë¦¬.md) â€” ë‹¤ìŒ ë‹¨ê³„: Q&A ë° ë§ˆë¬´ë¦¬

---

## ê·¸ë¦¼ ì¶œì²˜

ë³¸ ë¬¸ì„œì—ëŠ” ì¸ìš©í•œ ì™¸ë¶€ ê·¸ë¦¼ì´ ì—†ìŠµë‹ˆë‹¤.
