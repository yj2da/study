# 01. 오프닝

## 지난번(Day 2) 내용 복습

- **HDFS**: 대용량 데이터를 블록 단위로 여러 서버에 분산 저장. NameNode(메타데이터), DataNode(블록)
- **분산 파일 시스템**: NFS/CIFS → 중앙 집중식의 한계 → HDFS 등장
- **실습**: Docker/Podman Compose로 HDFS 클러스터 구성, `hdfs dfs` 명령어로 데이터 업로드·조회

→ 오늘은 저장된 데이터를 **분산 처리**하는 방법을 배웁니다. "저장 → 처리"로 이어지는 흐름입니다.

---

## 오늘의 학습 목표

오늘 강의를 마치면:
1. Apache Spark의 개념과 아키텍처를 이해한다
2. PySpark DataFrame API로 데이터를 로드하고 조작할 수 있다
3. 분산 환경에서 데이터 필터링, 집계, 정렬을 수행할 수 있다
4. HDFS와 Spark를 연동하여 데이터를 읽고 쓸 수 있다
5. Spark SQL을 사용하여 데이터를 분석할 수 있다
6. Transformation vs Action, Lazy Evaluation을 이해한다

---

## 📝 오늘의 진행 순서

1. [01_오프닝.md](01_오프닝.md) - 오프닝 및 Day2 복습 (지금 여기!)
2. [02_Spark_개념.md](02_Spark_개념.md) - Apache Spark란?
3. [03_Spark_아키텍처.md](03_Spark_아키텍처.md) - Spark 아키텍처 이해
4. [04_PySpark_기본문법.md](04_PySpark_기본문법.md) - PySpark 기본 문법
5. [05_PySpark_환경설정.md](05_PySpark_환경설정.md) - PySpark 환경 설정
6. [06_데이터_로드_기본조작.md](06_데이터_로드_기본조작.md) - 데이터 로드 및 기본 조작
7. [07_필터링_집계.md](07_필터링_집계.md) - 필터링 및 집계 실습
8. [08_HDFS_Spark_연동.md](08_HDFS_Spark_연동.md) - HDFS와 Spark 연동
9. [09_Spark_SQL.md](09_Spark_SQL.md) - Spark SQL 사용법
10. [10_실전_데이터분석.md](10_실전_데이터분석.md) - 실전 데이터 분석
11. [11_QA_마무리.md](11_QA_마무리.md) - Q&A 및 마무리

---

## 준비물 확인

- [ ] Day 1, 2에서 구성한 Docker/Podman 환경
- [ ] **Day2에서 생성한 `data-engineering-network`** (필수)
- [ ] **Day2 HDFS 환경 실행 중** (컨테이너 유지)
- [ ] Python 기본 문법 이해
- [ ] SQL 기본 문법 이해 (SELECT, WHERE, GROUP BY, ORDER BY 등)
- [ ] 최소 메모리 6GB 이상 (Spark + HDFS 동시 실행)
- [ ] 최소 20GB 여유 디스크 공간
- [ ] 브라우저 (Spark Web UI, Jupyter Notebook 접속용)

---

## 중요 사항

### Day 2 환경 확인 필수

Day3에서 Spark가 HDFS 데이터를 읽으므로:

```bash
# 네트워크 확인
podman network ls | grep data-engineering

# HDFS 컨테이너 확인
podman ps | grep namenode
podman ps | grep datanode

# HDFS 데이터 확인
podman exec -it namenode hdfs dfs -ls /user/data/
```

위 명령어가 정상 작동해야 Day3 실습이 가능합니다.
