# 08. HDFSì™€ Spark ì—°ë™

## ì‹¤ìŠµ ëª©í‘œ

Day2ì—ì„œ HDFSì— ì—…ë¡œë“œí•œ ë°ì´í„°ë¥¼ **Sparkë¡œ ì½ì–´ì„œ ë¶„ì„**í•©ë‹ˆë‹¤.

---

## ì‚¬ì „ ì¤€ë¹„

### 1. ë„¤íŠ¸ì›Œí¬ í™•ì¸

Day2ì—ì„œ `data-engineering-network`ë¥¼ ìƒì„±í–ˆìœ¼ë¯€ë¡œ, Day3 Spark ì»¨í…Œì´ë„ˆë„ ìë™ìœ¼ë¡œ ê°™ì€ ë„¤íŠ¸ì›Œí¬ì— ì—°ê²°ë©ë‹ˆë‹¤.

```bash
# ë„¤íŠ¸ì›Œí¬ í™•ì¸
podman network ls | grep data-engineering
# ë˜ëŠ”: docker network ls | grep data-engineering

# ë„¤íŠ¸ì›Œí¬ì— ì—°ê²°ëœ ì»¨í…Œì´ë„ˆ í™•ì¸
podman network inspect data-engineering-network
# ë˜ëŠ”: docker network inspect data-engineering-network
```

ì˜ˆìƒ ì¶œë ¥: `namenode`, `datanode`, `spark-master`, `spark-worker`, `pyspark-notebook`ì´ ëª¨ë‘ ë³´ì—¬ì•¼ í•©ë‹ˆë‹¤.

> ğŸ’¡ **ë„¤íŠ¸ì›Œí¬ê°€ ì—†ë‹¤ë©´?**
> - Day2 ì‹¤ìŠµì„ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”. [05_HDFS_í™˜ê²½êµ¬ì„±.md](../Day2/05_HDFS_í™˜ê²½êµ¬ì„±.md)ì—ì„œ ë„¤íŠ¸ì›Œí¬ë¥¼ ìƒì„±í–ˆì–´ì•¼ í•©ë‹ˆë‹¤.
> - ë„¤íŠ¸ì›Œí¬ê°€ ì—†ìœ¼ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ìƒì„±:
>   ```bash
>   podman network create data-engineering-network
>   # Day2, Day3 ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘ í•„ìš”
>   ```

### 2. HDFS ìƒíƒœ ë° ê¶Œí•œ ì„¤ì •

```bash
# HDFS ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
podman ps | grep namenode
podman ps | grep datanode

# HDFSì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
podman exec -it namenode hdfs dfs -ls /user/data/

# Spark(jovyan ì‚¬ìš©ì)ê°€ ì“¸ ìˆ˜ ìˆë„ë¡ ê¶Œí•œ ë³€ê²½
podman exec -it namenode hdfs dfs -chmod -R 777 /user/data
# ë˜ëŠ”: docker exec -it namenode hdfs dfs -chmod -R 777 /user/data
```

ì˜ˆìƒ ì¶œë ¥:
```
Found 1 items
-rw-r--r--   3 root supergroup         70 2026-02-16 13:13 /user/data/sample_data.csv
```

> âš ï¸ **ì¤‘ìš”**: 
> - bde2020 HDFS ì´ë¯¸ì§€ëŠ” RPC í¬íŠ¸ë¡œ **8020**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ (9000 ì•„ë‹˜).
> - **ê¶Œí•œ ì„¤ì • í•„ìˆ˜**: SparkëŠ” `jovyan` ì‚¬ìš©ìë¡œ ì‹¤í–‰ë˜ë¯€ë¡œ HDFS ë””ë ‰í† ë¦¬ì— ì“°ê¸° ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.
> - ì‹¤ë¬´ì—ì„œëŠ” ì‚¬ìš©ìë³„ ë””ë ‰í† ë¦¬ë¥¼ ë§Œë“¤ì§€ë§Œ, ì‹¤ìŠµì—ì„œëŠ” ê°„ë‹¨íˆ `777` ê¶Œí•œ ì‚¬ìš©

---

## ì‹¤ìŠµ ë‹¨ê³„

### 0. ê¸°ì¡´ Spark ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ (ì„ íƒì‚¬í•­)

ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ë¦¬ì†ŒìŠ¤ê°€ ì œí•œì ì´ë¯€ë¡œ, ê¸°ì¡´ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì¢…ë£Œí•˜ê³  ìƒˆë¡œ ì‹œì‘í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

1. ë¸Œë¼ìš°ì €ì—ì„œ **http://localhost:8080** (Spark Master Web UI) ì ‘ì†
2. **Running Applications** ì„¹ì…˜ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ í™•ì¸
3. ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ë¦„ í´ë¦­ â†’ ìƒë‹¨ì˜ **Kill** ë²„íŠ¼ í´ë¦­
4. **Completed Applications**ë¡œ ì´ë™ í™•ì¸

![Spark Completed Applications](img/spark_completed_applications.png)

> ğŸ’¡ **ì™œ Kill í•´ì•¼ í•˜ë‚˜ìš”?**
> - ì‹¤ë¬´ì—ì„œëŠ” í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤ê°€ ì¶©ë¶„í•˜ì—¬ ì—¬ëŸ¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë™ì‹œì— ì‹¤í–‰ ê°€ëŠ¥
> - ë¡œì»¬ í™˜ê²½(ë…¸íŠ¸ë¶/PC)ì—ì„œëŠ” ë©”ëª¨ë¦¬ì™€ CPUê°€ ì œí•œì ì´ë¯€ë¡œ ê¸°ì¡´ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì¢…ë£Œí•˜ëŠ” ê²ƒì´ ì¢‹ìŒ
> - íŠ¹íˆ Day2 HDFSì™€ Day3 Sparkë¥¼ ë™ì‹œì— ì‹¤í–‰í•˜ë©´ ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ê°€ëŠ¥

### 1. Sparkì—ì„œ HDFS ë°ì´í„° ì½ê¸°

ì‹ ê·œ Jupyter Notebookì—ì„œ ë‹¤ìŒ ì½”ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

```python
from pyspark.sql import SparkSession

# SparkSession ìƒì„±
spark = SparkSession.builder \
    .appName("HDFS_Spark_Integration") \
    .master("spark://spark-master:7077") \
    .getOrCreate()

# HDFSì—ì„œ ë°ì´í„° ì½ê¸°
# ê°™ì€ ë„¤íŠ¸ì›Œí¬ì— ìˆìœ¼ë¯€ë¡œ ì»¨í…Œì´ë„ˆëª…(namenode) ì‚¬ìš© ê°€ëŠ¥
df = spark.read.csv("hdfs://namenode:8020/user/data/sample_data.csv", header=True, inferSchema=True)

# ë°ì´í„° í™•ì¸
df.show()
df.printSchema()
```

> ğŸ’¡ **HDFS URI í˜•ì‹**: 
> - `hdfs://namenode:8020/ê²½ë¡œ` í˜•ì‹ìœ¼ë¡œ HDFS íŒŒì¼ì— ì ‘ê·¼í•©ë‹ˆë‹¤.
> - `namenode`ëŠ” ê°™ì€ ë„¤íŠ¸ì›Œí¬(`data-engineering-network`)ì— ìˆëŠ” NameNode ì»¨í…Œì´ë„ˆëª…
> - bde2020 ì´ë¯¸ì§€ëŠ” RPC í¬íŠ¸ë¡œ **8020**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ (Hadoop 3.x)

![spark_read_hdfs](img/spark_read_hdfs.png)

### 2. ë°ì´í„° ë¶„ì„

```python
from pyspark.sql import functions as F

# ë„ì‹œë³„ ì¸ì› ìˆ˜
df.groupBy("city").count().show()

# í‰ê·  ë‚˜ì´
df.agg(F.avg("age").alias("avg_age")).show()

# ë‚˜ì´ê°€ 30 ì´ìƒì¸ ì‚¬ëŒ
df.filter(df.age >= 30).show()
```

### 3. ê²°ê³¼ë¥¼ HDFSì— ì €ì¥

```python
# ë„ì‹œë³„ ì§‘ê³„ ê²°ê³¼ë¥¼ HDFSì— ì €ì¥
city_stats = df.groupBy("city").agg(
    F.count("id").alias("count"),
    F.avg("age").alias("avg_age")
)

# HDFSì— CSVë¡œ ì €ì¥
city_stats.write.csv("hdfs://namenode:8020/user/data/output/city_stats", header=True, mode="overwrite")

# ì €ì¥ í™•ì¸
spark.read.csv("hdfs://namenode:8020/user/data/output/city_stats", header=True).show()
```

![spark_hdfs_save](img/spark_hdfs_save.png)

### 4. HDFS Web UIì—ì„œ í™•ì¸

ë¸Œë¼ìš°ì €ì—ì„œ **http://localhost:9870** ì ‘ì† í›„:
- "Browse the file system" í´ë¦­
- `/user/data/output/city_stats` ë””ë ‰í† ë¦¬ í™•ì¸
- ìƒì„±ëœ íŒŒì¼ë“¤ í™•ì¸ (part-00000-*.csv í˜•íƒœ)

![hdfs_output](img/hdfs_output.png)

### 5. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

**ë¬¸ì œ: "No live nodes contain block" ë˜ëŠ” "Failed to connect to DataNode"**

ì›ì¸: Sparkì™€ HDFSê°€ ì„œë¡œ ë‹¤ë¥¸ ë„¤íŠ¸ì›Œí¬ì— ìˆì–´ DataNodeì— ì§ì ‘ ì—°ê²° ë¶ˆê°€

í•´ê²°: **ë°˜ë“œì‹œ ê³µìš© ë„¤íŠ¸ì›Œí¬ ì„¤ì • í•„ìš”** (ìœ„ì˜ "1. ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ í•´ê²°" ì°¸ì¡°)

**ë¬¸ì œ: "Call to localhost:8020 failed - Connection refused"**

ì›ì¸: ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ `localhost`ëŠ” ìê¸° ìì‹ ì„ ê°€ë¦¬í‚´

í•´ê²°: `namenode:8020` ì‚¬ìš© (ê°™ì€ ë„¤íŠ¸ì›Œí¬ì— ìˆì„ ë•Œ)

**ë¬¸ì œ: "Port 9000: Connection refused"**

ì›ì¸: bde2020 ì´ë¯¸ì§€ëŠ” í¬íŠ¸ 8020ì„ ì‚¬ìš©í•¨ (Hadoop 3.x)

í•´ê²°: URIë¥¼ `hdfs://namenode:8020/...`ë¡œ ë³€ê²½

**ë¬¸ì œ: "Permission denied: user=jovyan, access=WRITE"**

ì›ì¸: SparkëŠ” `jovyan` ì‚¬ìš©ìë¡œ ì‹¤í–‰ë˜ëŠ”ë°, HDFS ë””ë ‰í† ë¦¬ê°€ `root` ì†Œìœ 

í•´ê²°:
```bash
# HDFS ë””ë ‰í† ë¦¬ ê¶Œí•œ ë³€ê²½
podman exec -it namenode hdfs dfs -chmod -R 777 /user/data

# ë˜ëŠ” jovyan ì‚¬ìš©ì ë””ë ‰í† ë¦¬ ìƒì„±
podman exec -it namenode hdfs dfs -mkdir -p /user/jovyan
podman exec -it namenode hdfs dfs -chown jovyan:jovyan /user/jovyan
```

**ë¬¸ì œ: ë„¤íŠ¸ì›Œí¬ ì„¤ì • í›„ì—ë„ ì—°ê²° ì•ˆ ë¨**

í™•ì¸ ì‚¬í•­:
```bash
# ë„¤íŠ¸ì›Œí¬ í™•ì¸
podman network ls
# data-engineering-networkê°€ ìˆëŠ”ì§€ í™•ì¸

# ì»¨í…Œì´ë„ˆê°€ ë„¤íŠ¸ì›Œí¬ì— ì—°ê²°ë˜ì—ˆëŠ”ì§€ í™•ì¸
podman network inspect data-engineering-network
# namenode, datanode, spark-master, spark-worker, pyspark-notebookì´ ëª¨ë‘ ìˆì–´ì•¼ í•¨
```

---

## ì²´í¬í¬ì¸íŠ¸

**"HDFSì˜ ë°ì´í„°ë¥¼ Sparkë¡œ ì½ê³ , ê²°ê³¼ë¥¼ ë‹¤ì‹œ HDFSì— ì €ì¥í–ˆë‚˜ìš”?"**

ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í™•ì¸:
```bash
podman exec -it namenode hdfs dfs -ls /user/data/output/city_stats
```

<details>
<summary>ì˜ˆìƒ ê²°ê³¼</summary>

```
Found 2 items
-rw-r--r--   3 root supergroup          0 2026-02-17 ... /user/data/output/city_stats/_SUCCESS
-rw-r--r--   3 root supergroup         XX 2026-02-17 ... /user/data/output/city_stats/part-00000-....csv
```

</details>

---

## í•µì‹¬ ê°œë… ì •ë¦¬

- **HDFS URI**: `hdfs://namenode:8020/ê²½ë¡œ` í˜•ì‹ìœ¼ë¡œ HDFS íŒŒì¼ ì ‘ê·¼ (bde2020 ì´ë¯¸ì§€ëŠ” í¬íŠ¸ 8020 ì‚¬ìš©)
- **ì»¨í…Œì´ë„ˆ ë„¤íŠ¸ì›Œí‚¹**: 
  - **NameNode ì ‘ê·¼**: ë©”íƒ€ë°ì´í„° ì¡°íšŒ (íŒŒì¼ ìœ„ì¹˜ ì •ë³´)
  - **DataNode ì ‘ê·¼**: ì‹¤ì œ ë°ì´í„° ì½ê¸°/ì“°ê¸° (ë°˜ë“œì‹œ ê°™ì€ ë„¤íŠ¸ì›Œí¬ í•„ìš”)
  - ì„œë¡œ ë‹¤ë¥¸ docker-composeì˜ ì»¨í…Œì´ë„ˆëŠ” **ê³µìš© ë„¤íŠ¸ì›Œí¬**ë¡œ ì—°ê²° í•„ìˆ˜
- **Spark â†” HDFS ì—°ë™**: SparkëŠ” HDFSë¥¼ ë„¤ì´í‹°ë¸Œë¡œ ì§€ì›í•˜ì—¬ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥
- **ë¶„ì‚° ì €ì¥**: Sparkì˜ write ê²°ê³¼ëŠ” ì—¬ëŸ¬ part íŒŒì¼ë¡œ ë¶„ì‚° ì €ì¥ë¨
- **ì‹¤ë¬´ í™œìš©**: HDFSì— ì›ë³¸ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³ , Sparkë¡œ ì²˜ë¦¬í•œ ê²°ê³¼ë¥¼ ë‹¤ì‹œ HDFSì— ì €ì¥í•˜ëŠ” íŒ¨í„´

---

## ì°¸ê³ 

- [09_Spark_SQL.md](09_Spark_SQL.md) â€” ë‹¤ìŒ ë‹¨ê³„: Spark SQL ì‹¤ìŠµ.
- [Spark with HDFS](https://spark.apache.org/docs/latest/hadoop-provided.html)

---

## ê·¸ë¦¼ ì¶œì²˜

| íŒŒì¼ëª… | ì¶œì²˜ |
|--------|------|
| spark_completed_applications.png | Spark Master Web UI ìº¡ì²˜ (ì§ì ‘ ìº¡ì²˜) |
| spark_read_hdfs.png | Jupyter Notebook ì‹¤í–‰ ê²°ê³¼ (ì§ì ‘ ìº¡ì²˜) |
| spark_hdfs_save.png | Jupyter Notebook ì‹¤í–‰ ê²°ê³¼ (ì§ì ‘ ìº¡ì²˜) |
| hdfs_output.png | HDFS Web UI ìº¡ì²˜ (ì§ì ‘ ìº¡ì²˜) |
