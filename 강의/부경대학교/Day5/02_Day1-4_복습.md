# 02. Day 1~4 핵심 내용 리뷰

## Day 1 복습: 데이터 엔지니어링 & 빅데이터 기초

### 핵심 개념

**데이터 엔지니어링의 역할**
- 데이터의 "배관공" — 데이터가 흐르는 파이프라인 구축
- 데이터 수집 → 저장 → 처리 → 제공

**Linux 기본 명령어**
- `cd`, `ls`, `mkdir`, `cp`, `mv`, `rm`
- `cat`, `grep`, `find`
- 파이프(`|`), 리다이렉션(`>`, `>>`)

**Docker/Podman**
- 컨테이너: 격리된 환경에서 애플리케이션 실행
- 이미지: 컨테이너의 템플릿
- `docker run`, `docker ps`, `docker exec`

### 퀴즈

**Q: 데이터 엔지니어 vs 데이터 사이언티스트 차이는?**

<details>
<summary>답변</summary>

- **데이터 엔지니어**: 데이터 파이프라인 구축, 인프라 관리, 데이터 품질 보장
- **데이터 사이언티스트**: 데이터 분석, 모델링, 인사이트 도출

간단히 말하면, 데이터 엔지니어는 "데이터를 준비"하고, 데이터 사이언티스트는 "데이터를 분석"합니다.

</details>

---

## Day 2 복습: 분산 시스템 & 스토리지

### 핵심 개념

**분산 파일 시스템의 필요성**
- 단일 서버로는 대용량 데이터 저장·처리 불가
- 여러 서버에 데이터를 분산 저장

**HDFS 아키텍처**
- **NameNode**: 메타데이터 관리 (파일 위치 정보)
- **DataNode**: 실제 데이터 저장
- **블록 단위 저장**: 기본 128MB 단위로 분할
- **복제**: 데이터 손실 방지 (기본 3개 복제본)

**HDFS 명령어**
- `hdfs dfs -ls`: 파일 목록 조회
- `hdfs dfs -put`: 파일 업로드
- `hdfs dfs -get`: 파일 다운로드
- `hdfs dfs -cat`: 파일 내용 출력

### 퀴즈

**Q: HDFS는 어떤 워크로드에 적합한가?**

<details>
<summary>답변</summary>

**적합한 워크로드**:
- **대용량 파일** 저장 (GB~TB 단위)
- **순차 읽기** 중심 (로그 분석, 배치 처리)
- **Write Once, Read Many** (한 번 쓰고 여러 번 읽기)

**부적합한 워크로드**:
- **작은 파일** 대량 저장 (메타데이터 오버헤드)
- **랜덤 읽기/쓰기** (데이터베이스 워크로드)
- **실시간 처리** (지연 시간 높음)

</details>

---

## Day 3 복습: Spark & 분산 처리

### 핵심 개념

**Apache Spark의 특징**
- **인메모리 처리**: MapReduce보다 100배 빠름
- **통합 엔진**: 배치, 스트리밍, SQL, ML, 그래프 처리
- **다양한 언어 지원**: Scala, Java, Python, R

**RDD vs DataFrame**
- **RDD** (Resilient Distributed Dataset): 저수준 API, 유연함
- **DataFrame**: 고수준 API, SQL 최적화, 사용하기 쉬움

**Transformation vs Action**
- **Transformation**: 새로운 RDD/DataFrame 생성 (Lazy Evaluation)
  - `select()`, `filter()`, `groupBy()`, `join()`
- **Action**: 결과 계산 및 반환 (실제 실행)
  - `show()`, `count()`, `collect()`, `write()`

**PySpark 기본 연산**
- 데이터 읽기: `spark.read.csv()`, `spark.read.parquet()`
- 데이터 변환: `withColumn()`, `filter()`, `groupBy().agg()`
- 데이터 저장: `write.csv()`, `write.parquet()`

### 퀴즈

**Q: Lazy Evaluation이 왜 중요한가?**

<details>
<summary>답변</summary>

**Lazy Evaluation (지연 실행)**:
- Transformation은 즉시 실행되지 않고, **실행 계획만 기록**
- Action이 호출될 때 **전체 실행 계획을 최적화**하여 한 번에 실행

**장점**:
1. **최적화**: 불필요한 연산 제거, 실행 순서 재배치
2. **효율성**: 중간 결과를 메모리에 저장하지 않음
3. **성능**: 전체 파이프라인을 한 번에 실행하여 I/O 최소화

**예시**:
```python
df.filter(col("age") > 20).select("name").show()
```
→ Spark는 `filter`와 `select`를 최적화하여 필요한 컬럼만 읽고, 필터링 후 선택

</details>

---

## Day 4 복습: 데이터 처리 흐름 만들기

### 핵심 개념

**배치 처리와 파이프라인**
- **배치 처리**: 일정 주기마다 쌓인 데이터를 한 번에 처리
- **파이프라인**: Source → Ingestion → Storage → Processing → Serving

**Data Warehouse vs Data Lake**
- **Data Warehouse** 📦: 쿠팡 물류 창고처럼 정리된 데이터, 빠른 검색
- **Data Lake** 🏞️: 호수처럼 거대하고 다양한 데이터, 형식 상관없이 저장

**ETL/ELT/ETLT**
- **ETL**: Extract → Transform → Load (중간 서버에서 변환)
- **ELT**: Extract → Load → Transform (목적지에서 변환)
- **ETLT**: Extract → **light Transform** → Load → **full Transform** (현대적 접근, 개인정보 보호)

**파이프라인 자동화 및 검증**
- 함수화: 재사용 가능한 파이프라인 함수
- 에러 처리: `try-except` + `traceback`
- 로깅: 단계별 진행 상황 기록
- 검증: Null 체크, 중복 체크, 비즈니스 로직 검증

### 퀴즈

**Q: 멱등성(Idempotency)이란?**

<details>
<summary>답변</summary>

**멱등성**: 같은 입력으로 여러 번 실행해도 **같은 결과**를 보장하는 성질

**중요한 이유**:
- 파이프라인 실패 시 **안전하게 재실행** 가능
- 중복 실행으로 인한 **데이터 중복 방지**

**예시**:
- ❌ **멱등하지 않음**: `INSERT INTO table VALUES (...)` → 재실행 시 중복 데이터
- ✅ **멱등함**: `INSERT OVERWRITE table ...` 또는 `UPSERT` → 재실행 시 같은 결과

**구현 방법**:
- `mode="overwrite"` 사용
- 타임스탬프 기반 출력 경로 (`output_{timestamp}`)
- 트랜잭션 ID 기반 중복 제거

</details>

---

## 핵심 정리

| Day | 주제 | 핵심 키워드 |
|-----|------|-------------|
| 1 | 기초 환경 구축 | Linux, Docker, 데이터 엔지니어 역할 |
| 2 | 분산 저장 | HDFS, NameNode, DataNode, 블록, 복제 |
| 3 | 분산 처리 | Spark, DataFrame, Transformation/Action, Lazy Evaluation |
| 4 | 파이프라인 구축 | ETL/ELT/ETLT, 자동화, 검증, 멱등성 |

---

## 참고

- [03_종합실습.md](03_종합실습.md) — 다음 단계: 전자상거래 로그 분석 파이프라인 구축.

---

## 그림 출처

본 문서에는 별도 이미지가 사용되지 않았습니다.
